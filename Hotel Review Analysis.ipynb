{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS translationKeys.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataset provided consists of three csv files, each with slightly different schema\n",
    "DATASET_1 = \"dataset/7282_1.csv\"\n",
    "DATASET_2 = \"dataset/Datafiniti_Hotel_Reviews.csv\"\n",
    "DATASET_3 = \"dataset/Datafiniti_Hotel_Reviews_Jun19.csv\"\n",
    "\n",
    "DATASETS_IN_USE = [DATASET_1, DATASET_2, DATASET_3]\n",
    "\n",
    "MIN_REVIEW_LENGTH = 0\n",
    "\n",
    "TRAIN_PROPORTION = 0.1\n",
    "\n",
    "USE_TRANSLATOR = True\n",
    "\n",
    "TRANSLATOR_OPTIONS = Enum(\"OPTIONS\", \"TRANSLATE REMOVE THRESHOLD\")\n",
    "TRANSLATOR_FUNCTIONALITY = TRANSLATOR_OPTIONS.TRANSLATE #options are \"translate\", which translates non-english text to english, and \n",
    "                                        # \"remove\", which drops records with non-english text from the training set\n",
    "TRANSLATOR_THRESHOLD=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-Preparation and Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(DATASET_1)\n",
    "\n",
    "#print(df)\n",
    "#df_1.head()\n",
    "\n",
    "df_1.boxplot(column=\"reviews.rating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1[\"reviews.rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1[df_1[\"reviews.rating\"] > 5].head()\n",
    "df_1[df_1[\"reviews.rating\"] == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the unique values in DATASET_1, it seems that, while most of the reviews are rated on an integer scale between 1 and 5, at least some of the reviews are rated from 1.0-10.0. These reviews should be removed or re-scaled (I have chosen to remove them, as there are comparatively very few of them and there may be differences between what a user means by choosing two equivalent ratings on the two scales - ie. a 8/10 rating may have different connotations on average to a 4/5 rating.)\n",
    "\n",
    "Note: It is impossible to tell which of the remaining reviews are actually rated out of 10 rather than 5 - however, assuming the distribution of ratings out of 10 is similar to the distribution for ratings out of 5, there should be very few of these and their effect on any statistical analysis should be minor.\n",
    "\n",
    "Additionally, a quick glance at the 0 ratings suggests that these are not real reviews, but some artifact of the scraping process, and so should be removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[df_1[\"reviews.rating\"].isin([1.0,2.0,3.0,4.0,5.0])]\n",
    "\n",
    "print(df_1[\"reviews.rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(DATASET_2)\n",
    "df_2.head()\n",
    "\n",
    "df_2.boxplot(column=\"reviews.rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_2[\"reviews.rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(DATASET_3)\n",
    "df_3.head()\n",
    "\n",
    "df_3.boxplot(column=\"reviews.rating\")\n",
    "print(df_3[\"reviews.rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the columns that are present in each of the three datasets, and make sure that there aren't any important columns\n",
    "# (review text, title, rating, etc.) that are functionally identical, but named differently\n",
    "columns_intersection = set.intersection(set(df_1.columns.values),set(df_2.columns.values), set(df_3.columns.values))\n",
    "print(columns_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now concatenate the three data sources on the schema intersection\n",
    "\n",
    "df = pd.concat([df_1, df_2, df_3], join=\"inner\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Translation of non-English reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses the google cloud translate API to detect the text language, returning the language and the confidence\n",
    "def detect_language(text):\n",
    "    client = translate.Client()\n",
    "    \n",
    "    detection = client.detect_language(text)\n",
    "    \n",
    "    language = detection[\"language\"]\n",
    "    \n",
    "    #because the API says that the confidence value is not always available, give a confidence value of 1 \n",
    "    # (full confidence) if the confidence is not returned\n",
    "    \n",
    "    confidence = detection[\"confidence\"] if \"confidence\" in detection else 1\n",
    "    \n",
    "    return language, confidence\n",
    "    \n",
    "#uses the google cloud translate API to\n",
    "print(detect_language(\"aaaaaaa\"))\n",
    "\n",
    "def translate_text(text, source_lang=\"en\", target_lang=\"en\"):\n",
    "    client = translate.Client()\n",
    "    \n",
    "    translation = client.translate(text, source_language=source_lang, target_language=target_lang)\n",
    "    \n",
    "    return translation[\"translatedText\"]\n",
    "    \n",
    "print(translate_text(\"aaaaaaa\", source_lang=\"es\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(df):\n",
    "    \n",
    "    df[\"detected_language\"].map(detect_language)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
